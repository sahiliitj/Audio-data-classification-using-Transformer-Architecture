# -*- coding: utf-8 -*-
"""Assignment 2 M21MA210 DL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eoP9JOa7JMW-ARHdxkOXisjpOmyJIe3_

# Data Loading, Preprocessing and Exploration
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install lightning

pip install wandb

import os
from pathlib import Path
import pandas as pd
import torchaudio
import zipfile
from torchaudio.transforms import Resample
import IPython.display as ipd
from matplotlib import pyplot as plt
from tqdm import tqdm
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
import torch

path = Path('/content/drive/MyDrive/')
df = pd.read_csv('/content/drive/MyDrive/meta/esc50.csv')

wavs = list(path.glob('audio/*'))
waveform, sample_rate = torchaudio.load(wavs[0])
print("Shape of waveform: {}".format(waveform.size()))
print("Sample rate of waveform: {}".format(sample_rate))
plt.figure()
plt.plot(waveform.t().numpy())
ipd.Audio(waveform, rate=sample_rate)

import torch.nn.functional as F
class CustomDataset(Dataset):
    def __init__(self, dataset, **kwargs):
        self.data_directory = kwargs["data_directory"]
        self.data_frame = kwargs["data_frame"]
        self.validation_fold = kwargs["validation_fold"]
        self.testing_fold = kwargs["testing_fold"]
        self.esc_10_flag = kwargs["esc_10_flag"]
        self.file_column = kwargs["file_column"]
        self.label_column = kwargs["label_column"]
        self.sampling_rate = kwargs["sampling_rate"]
        self.new_sampling_rate = kwargs["new_sampling_rate"]
        self.sample_length_seconds = kwargs["sample_length_seconds"]

        # Filter dataframe based on esc_10_flag and data_type
        if self.esc_10_flag:
            self.data_frame = self.data_frame.loc[self.data_frame['esc10'] == True]

        if dataset == "train":
            self.data_frame = self.data_frame.loc[
                (self.data_frame['fold'] != self.validation_fold) & (self.data_frame['fold'] != self.testing_fold)]
        elif dataset == "val":
            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.validation_fold]
        elif dataset == "test":
            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.testing_fold]

        # Get unique categories from the filtered dataframe
        self.categories = sorted(self.data_frame[self.label_column].unique())

        # Initialize lists to hold file names, labels, and folder numbers
        self.file_names = []
        self.labels = []

        # Initialize dictionaries for category-to-index and index-to-category mapping
        self.category_to_index = {}
        self.index_to_category = {}
        for i, category in enumerate(self.categories):
            self.category_to_index[category] = i
            self.index_to_category[i] = category

        # Populate file names and labels lists by iterating through the dataframe
        for ind in tqdm(range(len(self.data_frame))):
            row = self.data_frame.iloc[ind]
            file_path = self.data_directory / "audio" / row[self.file_column]
            self.file_names.append(file_path)
            self.labels.append(self.category_to_index[row[self.label_column]])
        self.resampler = torchaudio.transforms.Resample(self.sampling_rate, self.new_sampling_rate)

        # Window size for rolling window sample splits (unfold method)
        if self.sample_length_seconds == 2:
            self.window_size = self.new_sampling_rate * 2
            self.step_size = int(self.new_sampling_rate * 0.75)
        else:
            self.window_size = self.new_sampling_rate
            self.step_size = int(self.new_sampling_rate * 0.5)

    def __getitem__(self, index):
        path = self.file_names[index]
        audio_file = torchaudio.load(path, format=None, normalize=True)
        audio_tensor = self.resampler(audio_file[0])
        splits = audio_tensor.unfold(1, self.window_size, self.step_size)
        samples = splits.permute(1, 0, 2)
        return samples, self.labels[index]

    def __len__(self):
        return len(self.file_names)

class CustomDataModule(pl.LightningDataModule):
    def __init__(self, **kwargs):
        # Initialize the CustomDataModule with batch size, number of workers, and other parameters
        super().__init__()
        self.batch_size = kwargs["batch_size"]
        self.num_workers = kwargs["num_workers"]
        self.data_module_kwargs = kwargs

    def setup(self, stage=None):
        # Define datasets for training, validation, and testing during Lightning setup

        # If in 'fit' or None stage, create training and validation datasets
        if stage == 'fit' or stage is None:
            self.training_dataset = CustomDataset(dataset="train", **self.data_module_kwargs)
            self.validation_dataset = CustomDataset(dataset="val", **self.data_module_kwargs)

        # If in 'test' or None stage, create testing dataset
        if stage == 'test' or stage is None:
            self.testing_dataset = CustomDataset(dataset="test", **self.data_module_kwargs)

    def train_dataloader(self):
        # Return DataLoader for training dataset
        return DataLoader(self.training_dataset,
                          batch_size=self.batch_size,
                          shuffle=True,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def val_dataloader(self):
        # Return DataLoader for validation dataset
        return DataLoader(self.validation_dataset,
                          batch_size=self.batch_size,
                          shuffle=False,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def test_dataloader(self):
        # Return DataLoader for testing dataset
        return DataLoader(self.testing_dataset,
                          batch_size=32,
                          shuffle=False,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def collate_function(self, data):
        examples, labels = zip(*data)
        examples = torch.cat(examples)
        labels = torch.flatten(torch.tensor(labels))
        return examples, labels

# Data Setup
test_samp = 1
valid_samp = 4
batch_size = 32
num_workers =12
custom_data_module = CustomDataModule(batch_size=batch_size,
                                      num_workers=num_workers,
                                      data_directory=path,
                                      data_frame=df,
                                      validation_fold=valid_samp,
                                      testing_fold=test_samp,
                                      esc_10_flag=True,
                                      file_column='filename',
                                      label_column='category',
                                      sampling_rate=44100,
                                      new_sampling_rate=16000,
                                      sample_length_seconds=1
                                      )
custom_data_module.setup()

print('Label to the class: ', custom_data_module.training_dataset[0][1])  # this prints the class label
print('Data Sample Shape: ', custom_data_module.training_dataset[0][0].shape)  # this prints the shape of the sample (Frames, Channel, Features)

import matplotlib.pyplot as plt
cls_dist = custom_data_module.training_dataset.data_frame['category'].value_counts()
colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6', '#ff6666', '#c2f0c2', '#c2f0f0', '#ffcc00']
plt.figure(figsize=(8, 6))
plt.pie(cls_dist, labels=cls_dist.index, autopct='%1.1f%%', startangle=140, colors=colors)
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)
plt.title('Class Distribution (10 Classes)', fontsize=10, fontweight='bold')
plt.axis('equal')
print("\n \n")
plt.show()

import librosa.display
for i in range(5):
    path = custom_data_module.training_dataset.file_names[i]
    waveform, _ = torchaudio.load(path)
    plt.figure(figsize=(10, 2))
    librosa.display.waveshow(waveform.numpy()[0], sr=custom_data_module.training_dataset.new_sampling_rate)
    plt.title(f'Waveform for Sample datapoint {i+1}')
    plt.show()

"""# Model 1 of Architecture 1"""

import torch
import torch.nn as nn

class Arch1(nn.Module):
    def __init__(self, labs):
        super(Arch1, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout(0.2)
        self.bn1 = nn.BatchNorm1d(32)
        self.bn2 = nn.BatchNorm1d(64)
        self.bn3 = nn.BatchNorm1d(128)
        self.fc = nn.Linear(64*4000, labs)
    def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.pool(x)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.pool(x)
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.pool(x)
        x = self.dropout(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        x = x.view(-1, 9, x.size(1)).mean(dim=1)
        return x

import torch
torch.cuda.reset_max_memory_allocated()
torch.cuda.empty_cache()

import torch
import torch.nn as nn
import torch.optim as optim
mod1 = Arch1(10)
loss_func = nn.CrossEntropyLoss()
optimizer = optim.Adam(mod1.parameters(), lr=0.001)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
mod1.to(device)
loss_func.to(device)
optimizer = torch.optim.Adam(mod1.parameters(), lr=0.001)

pip install wandb

"""Training Phase of the model"""

import torch
import torch.nn as nn
import wandb
def training(mod, cr, optimizer, train_loader, val_loader, epochs,device):
    mod.to(device)
    for epoch in range(epochs):
        mod.train()
        run_loss, train_corr, train_total = 0.0, 0, 0
        for i, j in train_loader:
            i, j = i.to(device), j.to(device)
            optimizer.zero_grad()
            outs = mod(i)
            loss = cr(outs, j)
            loss.backward()
            optimizer.step()
            run_loss += loss.item() * i.size(0)
            _, preds = torch.max(outs, 1)
            train_total += j.size(0)
            train_corr += (preds == j).sum().item()
        ep_loss = run_loss / len(train_loader.dataset)
        ep_tr_acc = 100 * train_corr / train_total
    # Validation Phase
        mod.eval()
        run_valloss,correct_val,total_val = 0.0,0,0
        with torch.no_grad():
            for i, j in val_loader:
                i, j = i.to(device), j.to(device)
                outs = mod(i)
                loss = cr(outs, j)
                run_valloss += loss.item() * i.size(0)
                _, preds = torch.max(outs, 1)
                total_val += j.size(0)
                correct_val += (preds == j).sum().item()
        ep_val_loss = run_valloss / len(val_loader.dataset)
        ep_val_acc = 100 * correct_val / total_val
        wandb.log({
            "Train Accuracy": ep_tr_acc,
            "Validation Accuracy": ep_val_acc,
            "Training Loss": ep_loss,
            "Validation Loss": ep_val_loss
        })
        print(f'Epoch ({epoch+1}):\n  Training Accuracy: {ep_tr_acc:.2f}%,  Validation Accuracy: {ep_val_acc:.2f}% ,  Train Loss: {ep_loss:.4f}, Val Loss: {ep_val_loss:.4f}')

import torch
import torch.nn as nn
import torch.optim as optim
mod1 = Arch1(10)
loss_func = nn.CrossEntropyLoss()
optimizer = optim.Adam(mod1.parameters(), lr=0.001)

wandb.init(project="Assignment 2", entity="sahil-2")
training(mod1, loss_func, optimizer,custom_data_module.train_dataloader(),custom_data_module.val_dataloader(),100,device)
wandb.finish()

""" Testing Phase of the model and various reports"""

def testing(model,cr, loader,device):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    loss_test, corr, total = 0.0, 0, 0
    with torch.no_grad():
        for i, j in loader:
            i, j = i.to(device), j.to(device)
            outs = model(i)
            loss =cr(outs, j)
            loss_test += loss.item() * i.size(0)
            _, predicted = torch.max(outs, 1)
            total += j.size(0)
            corr += (predicted == j).sum().item()
    loss_test /= len(loader)
    acc_test = 100 * corr / total
    return loss_test, acc_test
loss_test_set, acc_test_set  = testing(mod1, loss_func, custom_data_module.test_dataloader(),device)
print("Accuracy for test data set: ", acc_test_set)

import numpy as np
import torch
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, f1_score
def model_reports(model, loader, labels):
    model.eval()
    pred_labs = []
    labs_true = []
    with torch.no_grad():
        for i, j in loader:
            i, j = i.to(device), j.to(device)
            outs = model(i)
            _, preds = torch.max(outs, 1)
            pred_labs.extend(preds.cpu().numpy())
            labs_true.extend(j.cpu().numpy())
    labs_true = np.array(labs_true)
    pred_labs = np.array(pred_labs)
    acc = accuracy_score(labs_true, pred_labs)
    cmat = confusion_matrix(labs_true, pred_labs)
    rep = classification_report(labs_true, pred_labs)
    score_f1 = f1_score(labs_true, pred_labs, average=None)
    falsepred = []
    true_pred = []
    roc_auc_list = []
    for i in range(labels):
        fpr, tpr, _ = roc_curve(labs_true == i, pred_labs == i)
        roc_auc = auc(fpr, tpr)
        falsepred.append(fpr)
        true_pred.append(tpr)
        roc_auc_list.append(roc_auc)

    return acc, cmat, rep, score_f1, falsepred, true_pred, roc_auc_list

accuracy, cm, cr, f1_scores, falsepred, true_pred, roc_auc_list = model_reports(mod1, custom_data_module.test_dataloader(),10)

# Print the results
print(f'Accuracy of the model on test data set : {100*accuracy:.3f}%')
print("\n")
print(f'Classification Report:\n{cr}')
# F1-scores
plt.figure(figsize=(7,7))
bar_colors = plt.cm.viridis(np.linspace(0, 1, len(f1_scores)))
bars = plt.bar(range(len(f1_scores)), f1_scores, color=bar_colors, width=0.5)
plt.title("F1-Scores corresponding to each class", fontsize=10)
plt.ylabel("F1-Score", fontsize=8)
plt.xlabel("Class label", fontsize=8)
plt.xticks(range(len(f1_scores)), labels=[f" {i}" for i in range(len(f1_scores))], fontsize=8)
plt.yticks(fontsize=6)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(6, 6))
sns.heatmap(cm,cmap = 'YlOrBr', annot=True, fmt='d')
plt.xlabel('Prediction by the model')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for the test dataset')
plt.show()

# ROC Curves and AUC scores
plt.figure(figsize=(8, 6))
for i in range(10):
    plt.plot(falsepred[i], true_pred[i], lw=2, label=f'ROC ( Class {i}:AUC= {roc_auc_list[i]:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve along with AUC scores for each class')
plt.legend(loc="lower right")
plt.show()

train_par = sum(p.numel() for p in mod1.parameters() if p.requires_grad)
total_par = sum(p.numel() for p in mod1.parameters())
print(f'Total parameters in the model: {total_par}')
print(f'Total trainable parameters in the model: {train_par}')
print(f'Total non-trainable parameters in the model: {total_par-train_par }')

torch.save(mod1, '/content/drive/MyDrive/Arch1.pt')

"""# Architecture 2"""

import torch
import torch.nn as nn
class Feature_Extract(nn.Module):
    def __init__(self, labs):
        super(Feature_Extract, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout(0.2)
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = torch.relu(self.conv3(x))
        x = self.pool(x)
        x = self.dropout(x)
        x = x.view(-1, 9, x.size(1), x.size(2)).mean(dim=1)
        return x

class Sequential_Feedfwd(nn.Module):
    def __init__(self, model_dimension,feed_dimension):
        super(Sequential_Feedfwd, self).__init__()
        self.full1 = nn.Linear(model_dimension,feed_dimension)
        self.full2 = nn.Linear(feed_dimension, model_dimension)
        self.activation = nn.ReLU()
    def forward(self, x):
        x = self.full1(x)
        x = self.activation(x)
        x = self.full2(x)
        return x

import math
class Attention_by_heads(nn.Module):
    def __init__(self,model_dim, heads):
        super(Attention_by_heads, self).__init__()
        assert model_dim % heads == 0,"number of heads must divide the dimension of your model"
        self.model_dim =model_dim
        self.heads = heads
        self.D =model_dim // heads
        self.W_k = nn.Linear(model_dim,model_dim)
        self.W_q = nn.Linear(model_dim,model_dim)
        self.W_o = nn.Linear(model_dim,model_dim)
        self.W_v = nn.Linear(model_dim,model_dim)
    def distribute_to_heads(self, x):
        batch_size, seq_length,model_dim = x.size()
        x = x.view(batch_size, seq_length, self.heads, self.D)
        x = x.transpose(1, 2)
        return x
    def compute_align(self, query, key, vals):
        aligns = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.D)
        attn_vals = torch.softmax(aligns, dim=-1)
        result = torch.matmul(attn_vals, vals)
        return result
    def merge_all(self, x):
        bsize, _, seq_length, D = x.size()
        x = x.transpose(1, 2)
        x = x.contiguous()
        x = x.view(bsize, seq_length, self.model_dim)
        return x
    def forward(self, query, key, value):
        key = self.distribute_to_heads(self.W_k(key))
        query = self.distribute_to_heads(self.W_q(query))
        value = self.distribute_to_heads(self.W_v(value))
        attns = self.compute_align(query, key, value)
        output = self.W_o(self.merge_all(attns))
        return output

class Single_tfm_layer(nn.Module):
    def __init__(self, d_model, num_heads, feed_dim):
        super(Single_tfm_layer, self).__init__()
        self.feedforward = Sequential_Feedfwd(d_model, feed_dim)
        self.attn_mh = Attention_by_heads(d_model, num_heads)
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
    def forward(self, x):
        attns = self.attn_mh(x, x, x)
        x = x + attns
        x = self.ln1(x)
        feeds = self.feedforward(x)
        x = x + feeds
        x = self.ln2(x)
        return x

class Encode_Pos(nn.Module):
    def __init__(self, model_dim, sequence_length):
        super(Encode_Pos, self).__init__()
        pe = torch.zeros(sequence_length, model_dim)
        location = torch.arange(0, sequence_length, dtype=torch.float).unsqueeze(1)
        add_term = torch.exp(torch.arange(0, model_dim, 2).float() * -(math.log(10000.0) / model_dim))
        pe[:, 1::2] = torch.cos(location * add_term)
        pe[:, 0::2] = torch.sin(location * add_term)
        self.register_buffer('pe', pe.unsqueeze(0))
    def forward(self, x):
        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)
        return x

class Block_Encoder(nn.Module):
    def __init__(self, layers, model_dim,heads,feed_dim, length_seq):
        super(Block_Encoder, self).__init__()
        self.ec_layer = nn.Sequential(*[
            Single_tfm_layer(model_dim,heads,feed_dim) for _ in range(layers)
        ])
        self.po_enc =Encode_Pos(model_dim, length_seq)
    def forward(self, x):
        x = self.po_enc(x)
        for layer in self.ec_layer:
            x = layer(x)
        return x

class Combined_Trans_Classifier(nn.Module):
    def __init__(self, n_layers, model_dim, heads, feed_dim, length_seq, labels):
        super(Combined_Trans_Classifier, self).__init__()
        self.tfm_ecd = Block_Encoder(n_layers, model_dim, heads, feed_dim, length_seq)
        self.clf = nn.Linear(model_dim, labels)
        self.tok_cls = nn.Parameter(torch.randn(1, 1, model_dim))
        self.extract = Feature_Extract(10)
    def forward(self, x):
        x = self.extract(x)
        tok_cls = self.tok_cls.expand(x.size(0), -1, -1)
        x = torch.cat([tok_cls, x], dim=1)
        x = self.tfm_ecd(x)
        x = self.clf(x[:, 0, :])
        return x

import torch
import torch.nn as nn
import wandb
def training_phase(mod, cr, optimizer, train_loader, val_loader, epochs,device):
    mod.to(device)
    for epoch in range(epochs):
        mod.train()
        run_loss, train_corr, train_total = 0.0, 0, 0
        for i, j in train_loader:
            i, j = i.to(device), j.to(device)
            optimizer.zero_grad()
            outs = mod(i)
            loss = cr(outs, j)
            loss.backward()
            optimizer.step()
            run_loss += loss.item() * i.size(0)
            _, preds = torch.max(outs, 1)
            train_total += j.size(0)
            train_corr += (preds == j).sum().item()
        ep_loss = run_loss / len(train_loader.dataset)
        ep_tr_acc = 100 * train_corr / train_total
    # Validation Phase
        mod.eval()
        run_valloss,correct_val,total_val = 0.0,0,0
        with torch.no_grad():
            for i, j in val_loader:
                i, j = i.to(device), j.to(device)
                outs = mod(i)
                loss = cr(outs, j)
                run_valloss += loss.item() * i.size(0)
                _, preds = torch.max(outs, 1)
                total_val += j.size(0)
                correct_val += (preds == j).sum().item()
        ep_val_loss = run_valloss / len(val_loader.dataset)
        ep_val_acc = 100 * correct_val / total_val
        wandb.log({
            "Train Accuracy": ep_tr_acc,
            "Validation Accuracy": ep_val_acc,
            "Training Loss": ep_loss,
            "Validation Loss": ep_val_loss
        })
        print(f'Epoch ({epoch+1}):\n  Training Accuracy: {ep_tr_acc:.2f}%,  Validation Accuracy: {ep_val_acc:.2f}% ,  Training Loss: {ep_loss:.4f}, Val Loss: {ep_val_loss:.4f}')

def model_creation(layers, model_dim, heads, feed_dim, seq_len, labs):
    model = Combined_Trans_Classifier(layers, model_dim, heads, feed_dim, seq_len, labs).to(device)  # Define your transformer model
    loss = nn.CrossEntropyLoss()
    opt = optim.Adam(model.parameters(), lr=0.001)
    return model,loss,opt

def testing_for_model(model,cr, loader,device):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    loss_test, corr, total = 0.0, 0, 0
    with torch.no_grad():
        for i, j in loader:
            i, j = i.to(device), j.to(device)
            outs = model(i)
            loss =cr(outs, j)
            loss_test += loss.item() * i.size(0)
            _, preds = torch.max(outs, 1)
            total += j.size(0)
            corr += (preds == j).sum().item()
    loss_test /= len(loader)
    acc_test = 100 * corr / total
    return loss_test, acc_test

import numpy as np
import torch
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, f1_score
def model_reports(model, loader, labels):
    model.eval()
    pred_labs = []
    labs_true = []
    with torch.no_grad():
        for i, j in loader:
            i, j = i.to(device), j.to(device)
            outs = model(i)
            _, preds = torch.max(outs, 1)
            pred_labs.extend(preds.cpu().numpy())
            labs_true.extend(j.cpu().numpy())
    labs_true = np.array(labs_true)
    pred_labs = np.array(pred_labs)
    acc = accuracy_score(labs_true, pred_labs)
    cmat = confusion_matrix(labs_true, pred_labs)
    rep = classification_report(labs_true, pred_labs)
    score_f1 = f1_score(labs_true, pred_labs, average=None)
    falsepred = []
    true_pred = []
    roc_auc_list = []
    for i in range(labels):
        fpr, tpr, _ = roc_curve(labs_true == i, pred_labs == i)
        roc_auc = auc(fpr, tpr)
        falsepred.append(fpr)
        true_pred.append(tpr)
        roc_auc_list.append(roc_auc)
    return acc, cmat, rep, score_f1, falsepred, true_pred, roc_auc_list

def accuracy_report(accuracy,report):
    print(f"Accuracy of the model on test dataset is: {accuracy:.2f} %")
    print("The classification report for the model is:\n", report)

def plot_f1(f1_scores):
    plt.figure(figsize=(7,7))
    bar_colors = plt.cm.viridis(np.linspace(0, 1, len(f1_scores)))
    bars = plt.bar(range(len(f1_scores)), f1_scores, color=bar_colors, width=0.5)
    plt.title("F1-Scores corresponding to each class", fontsize=10)
    plt.ylabel("F1-Score", fontsize=8)
    plt.xlabel("Class label", fontsize=8)
    plt.xticks(range(len(f1_scores)), labels=[f" {i}" for i in range(len(f1_scores))], fontsize=8)
    plt.yticks(fontsize=6)
    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
def cmat_generation(matrix):
    plt.figure(figsize=(6, 6))
    sns.heatmap(matrix,cmap = 'YlOrBr', annot=True, fmt='d')
    plt.xlabel('Prediction by the model')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix for the test dataset')
    plt.show()

# ROC Curves and AUC scores
def plot_roc_curve(roc_list,false_pred,true_pred):
    plt.figure(figsize=(8, 6))
    for i in range(10):
        plt.plot(false_pred[i], true_pred[i], lw=2, label=f'ROC ( Class {i}:AUC= {roc_list[i]:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve along with AUC scores for each class')
    plt.legend(loc="lower right")
    plt.show()

def parameters(model):
    train_par = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_par = sum(p.numel() for p in model.parameters())
    print(f'Total parameters in the model: {total_par}')
    print(f'Total trainable parameters in the model: {train_par}')
    print(f'Total non-trainable parameters in the model: {total_par-train_par }')

"""# Model Number 2 with k = 1 in Architecture 2"""

import torch
import torch.nn as nn
import torch.optim as optim
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
mod2,loss_func2, optimizer2 = model_creation(3,2000,1,32,2000,10)
wandb.init(project="Assignment 2", entity="sahil-2")
training_phase(mod2, loss_func2, optimizer2,custom_data_module.train_dataloader(),custom_data_module.val_dataloader(),100,device)
wandb.finish()

loss_test_set2, acc_test_set2  = testing_for_model(mod2, loss_func2, custom_data_module.test_dataloader(),device)
print("Accuracy for test data set: ", acc_test_set2)

accuracy2, cm2, rep2, fscore2, falsepred2, truepred2, roc_auc_list2 = model_reports(mod2, custom_data_module.test_dataloader(),10)

accuracy_report(accuracy2, rep2)

plot_f1(fscore2)

plot_roc_curve(roc_auc_list2, falsepred2, truepred2)

cmat_generation(cm2)

parameters(mod2)

import torch
torch.cuda.empty_cache()

"""# Model Number 3 with k = 2 in Architecture 2"""

import torch
import torch.nn as nn
import torch.optim as optim
hardware = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
mod3,loss_func3, optimizer3 = model_creation(3,2000,2,32,2000,10)
wandb.init(project="Assignment 2", entity="sahil-2")
training_phase(mod3, loss_func3, optimizer3,custom_data_module.train_dataloader(),custom_data_module.val_dataloader(),100,hardware)
wandb.finish()

loss_test_set3, acc_test_set3  = testing_for_model(mod3, loss_func3, custom_data_module.test_dataloader(),device)
print("Accuracy for test data set: ", acc_test_set3)

accuracy3, cm3, rep3, fscore3, falsepred3, truepred3, roc_auc_list3 = model_reports(mod3, custom_data_module.test_dataloader(),10)

accuracy_report(accuracy3, rep3)

plot_f1(fscore3)

plot_roc_curve(roc_auc_list3, falsepred3, truepred3)

cmat_generation(cm3)

parameters(mod3)

import torch
torch.cuda.empty_cache()

"""# Model Number 4 with k = 4 in Architecture 2


"""

import torch
import torch.nn as nn
import torch.optim as optim
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
mod4,loss_func4, optimizer4 = model_creation(3,2000,4,32,2000,10)
wandb.init(project="Assignment 2", entity="sahil-2")
training_phase(mod4, loss_func4, optimizer4,custom_data_module.train_dataloader(),custom_data_module.val_dataloader(),100,device)
wandb.finish()

loss_test_set4, acc_test_set4  = testing_for_model(mod4, loss_func4, custom_data_module.test_dataloader(),device)
print("Accuracy for test data set: ", acc_test_set4)

accuracy4, cm4, rep4, fscore4, falsepred4, truepred4, roc_auc_list4 = model_reports(mod4, custom_data_module.test_dataloader(),10)

accuracy_report(accuracy4, rep4)

plot_f1(fscore4)

plot_roc_curve(roc_auc_list4, falsepred4, truepred4)

cmat_generation(cm4)

parameters(mod4)

import torch
torch.cuda.empty_cache()